{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inferencía y analisís de comunidades en Redes sociales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introducción\n",
        "\n",
        "## Problemática y Motivación\n",
        "\n",
        "La identificación de comunidades en redes sociales ha sido tradicionalmente abordada mediante el análisis de conexiones explícitas entre usuarios (redes de seguimiento, menciones, interacciones directas). Sin embargo, este enfoque presenta limitaciones significativas: requiere acceso a datos de conectividad que no siempre están disponibles, puede verse afectado por comportamientos artificiales o estratégicos, y no necesariamente refleja afinidades ideológicas o temáticas reales entre usuarios.\n",
        "\n",
        "## Propuesta Metodológica\n",
        "\n",
        "Este proyecto desarrolla una **metodología alternativa para la inferencia automática de comunidades** que se basa exclusivamente en el **análisis semántico del contenido textual** generado por los usuarios. La hipótesis central es que usuarios con afinidades similares tienden a expresarse sobre temas comunes utilizando vocabulario, marcos conceptuales y estructuras discursivas similares, creando patrones detectables mediante técnicas de procesamiento de lenguaje natural y aprendizaje automático.\n",
        "\n",
        "## Marco Teórico\n",
        "\n",
        "### Análisis Semántico Multidimensional\n",
        "\n",
        "La metodología propuesta reconoce que el contenido textual puede ser representado desde múltiples perspectivas complementarias:\n",
        "\n",
        "1. **Nivel Léxico**: Frecuencia y distribución de términos específicos\n",
        "2. **Nivel Temático**: Identificación de tópicos latentes en el corpus\n",
        "3. **Nivel Semántico**: Representaciones densas que capturan significado contextual\n",
        "\n",
        "### Clustering Ensemble\n",
        "\n",
        "Para maximizar la robustez de la identificación de comunidades, se implementa un enfoque de **clustering multi-view** que combina múltiples representaciones vectoriales del discurso. Esta estrategia mitiga las limitaciones inherentes de métodos individuales y aprovecha las fortalezas complementarias de diferentes aproximaciones.\n",
        "\n",
        "## Arquitectura Metodológica\n",
        "\n",
        "### 1. Pipeline de Vectorización Textual\n",
        "\n",
        "#### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "- Cuantifica la importancia de términos específicos en documentos individuales\n",
        "- Identifica vocabulario distintivo de diferentes grupos de usuarios\n",
        "- Proporciona interpretabilidad directa de las características más discriminativas\n",
        "\n",
        "#### Topic Modeling\n",
        "- **Non-negative Matrix Factorization (NMF)**: Descompone el corpus en temas latentes interpretables\n",
        "- **Alternativa**: Latent Dirichlet Allocation (LDA) para distribuciones probabilísticas de temas\n",
        "- Captura estructuras temáticas de alto nivel que trascienden términos individuales\n",
        "\n",
        "#### Document Embeddings\n",
        "- Representaciones vectoriales densas que capturan contexto semántico\n",
        "- **Aprendizaje supervisado**: Utiliza etiquetas disponibles (ej. afinidad política) para guiar la representación\n",
        "- **Implementación**: Redes neuronales con TensorFlow para optimización end-to-end\n",
        "\n",
        "### 2. Sistema de Clustering Multicapa\n",
        "\n",
        "#### Algoritmos Evaluados\n",
        "- **Clustering Aglomerativo**: Construye jerarquías naturales mediante fusión iterativa\n",
        "- **K-Means**: Optimiza particiones mediante minimización de varianza intra-cluster\n",
        "- **Clustering Espectral**: Detecta estructuras complejas mediante análisis de eigenvalores\n",
        "\n",
        "#### Votación Ponderada\n",
        "- Combina resultados de múltiples algoritmos y representaciones\n",
        "- Asigna pesos adaptativos basados en métricas de calidad (silhouette, cohesión interna)\n",
        "- Genera consenso robusto que trasciende limitaciones individuales\n",
        "\n",
        "### 3. Construcción de Redes de Similitud\n",
        "\n",
        "#### Matriz de Similitud\n",
        "- Cuantifica proximidad semántica entre usuarios basada en clustering\n",
        "- Incorpora múltiples métricas: coseno, euclidiana, correlación\n",
        "- Permite análisis de estructura comunitaria a diferentes escalas\n",
        "\n",
        "#### Grafo Ponderado\n",
        "- Transforma similitudes en red navegable\n",
        "- Facilita visualización y análisis topológico\n",
        "- Habilita aplicación de algoritmos de detección de comunidades en grafos\n",
        "\n",
        "## Ventajas Metodológicas\n",
        "\n",
        "### Independencia de Conectividad Explícita\n",
        "- No requiere datos de seguimiento o conexiones directas\n",
        "- Aplicable a plataformas con APIs restringidas\n",
        "- Revela afinidades implícitas no manifiestas en conexiones\n",
        "\n",
        "### Robustez y Generalización\n",
        "- Múltiples representaciones reducen dependencia de sesgos específicos\n",
        "- Ensemble clustering mejora estabilidad de resultados\n",
        "- Metodología transferible entre dominios y plataformas\n",
        "\n",
        "### Interpretabilidad\n",
        "- TF-IDF y topic modeling proporcionan insights semánticos directos\n",
        "- Comunidades pueden ser caracterizadas por vocabulario y temas distintivos\n",
        "- Facilita análisis cualitativo de resultados cuantitativos\n",
        "\n",
        "## Implementación Técnica\n",
        "\n",
        "### Stack Tecnológico\n",
        "- **Procesamiento de texto**: scikit-learn, NLTK, spaCy\n",
        "- **Machine Learning**: scikit-learn, TensorFlow\n",
        "- **Análisis de redes**: NetworkX, igraph\n",
        "- **Visualización**: Streamlit, Plotly, Matplotlib\n",
        "\n",
        "## Aplicaciones y Casos de Uso\n",
        "\n",
        "### Investigación Académica\n",
        "- Estudios de polarización y fragmentación social\n",
        "- Análisis de discurso político y formación de opinión\n",
        "- Investigación en comunicación mediada por computadora\n",
        "\n",
        "### Aplicaciones Comerciales\n",
        "- Segmentación de audiencias para marketing\n",
        "- Análisis de sentiment y brand monitoring\n",
        "- Personalización de contenido y recomendaciones\n",
        "\n",
        "### Monitoreo Social\n",
        "- Detección temprana de tendencias emergentes\n",
        "- Análisis de campañas de desinformación\n",
        "- Evaluación de políticas públicas de comunicación\n",
        "\n",
        "## Contribuciones y Futuras Direcciones\n",
        "\n",
        "Esta metodología representa una contribución significativa al campo de la minería de comunidades al demostrar que el análisis puramente textual puede revelar estructuras sociales complejas. Las direcciones futuras incluyen el análisis temporal de evolución comunitaria, y extensión a contenido multimodal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estructura de carpetas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        " ``` \n",
        "Estructura general del directorio:\n",
        "├── app # Aplicación para mostrar resultados de manera interactiva\n",
        "│   ├── .streamlit # Configuración de la app\n",
        "│   ├── app.py # Aplicación principal\n",
        "│   ├── components # Componentes\n",
        "│   ├── full_pages # Páginas\n",
        "│   ├── static \n",
        "│   │   └── plots # Plots generados\n",
        "│   ├── styling.py # Estilo\n",
        "│   └── utils # Utilidades\n",
        "├── data.csv # CSV con los datos de usuarios\n",
        "└── data_analysis.ipynb # Programa principal del proyecto, generación de plots, modelos, etc\n",
        " ``` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Empaquetado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de librerias \n",
        "from itertools import combinations  # Para generar combinaciones de elementos\n",
        "from IPython.display import display  # Para mostrar objetos en entornos IPython/Jupyter\n",
        "import matplotlib.colors as mcolors  # Para utilidades de colores de Matplotlib\n",
        "import matplotlib.pyplot as plt  # Para la creación de gráficos y visualizaciones\n",
        "import networkx as nx  # Para la creación, manipulación y estudio de estructuras de redes\n",
        "import nltk  # Para procesamiento de lenguaje natural\n",
        "from nltk.corpus import stopwords  # Para acceder a listas de palabras vacías (stopwords)\n",
        "import numpy as np  # Para trabajo con matrices y operaciones numéricas de alto rendimiento\n",
        "import os  # Para interactuar con el sistema operativo\n",
        "from os.path import join  # Para unir rutas de archivos de manera independiente del sistema operativo\n",
        "import pandas as pd  # Para manipulación y análisis de datos en estructuras tabulares\n",
        "import plotly  # Para la creación de gráficos interactivos y dashboards\n",
        "import plotly.graph_objects as go  # Para construir figuras de Plotly\n",
        "from plotly.subplots import make_subplots  # Para crear subplots en Plotly\n",
        "from matplotlib.patches import Patch  # Para crear parches en gráficos de Matplotlib (ej. leyendas)\n",
        "import random  # Para generar números aleatorios\n",
        "import re  # Para operaciones con expresiones regulares\n",
        "from scipy.optimize import linear_sum_assignment  # Para resolver el problema de asignación lineal\n",
        "from scipy.stats import gaussian_kde  # Para estimación de densidad de kernel gaussiana\n",
        "import seaborn as sns # Para graficar\n",
        "from sklearn.base import clone  # Para clonar estimadores de scikit-learn\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering  # Para algoritmos de clustering\n",
        "from sklearn.decomposition import NMF  # Para Factorización de Matrices No Negativas\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Para convertir colecciones de documentos de texto en matrices de conteo de tokens\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Para convertir colecciones de documentos de texto en matrices TF-IDF\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score  # Para métricas de evaluación de clustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # Para calcular la similitud del coseno entre vectores\n",
        "from sklearn.model_selection import ParameterSampler  # Para muestreo de hiperparámetros\n",
        "from sklearn.model_selection import train_test_split  # Para dividir datos en conjuntos de entrenamiento y prueba\n",
        "from sklearn.preprocessing import LabelEncoder  # Para codificar etiquetas categóricas en valores numéricos\n",
        "import tensorflow as tf  # Para aprendizaje automático y redes neuronales\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Para rellenar secuencias con la misma longitud\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # Para tokenización de texto\n",
        "from tqdm import tqdm  # Para mostrar barras de progreso en bucles\n",
        "from typing import Tuple  # Para anotaciones de tipo (tuplas y cualquier tipo)\n",
        "from wordcloud import WordCloud  # Para generar nubes de palabras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5afmJFoz2gI"
      },
      "source": [
        "# Obtención de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "UXr6YblBecIj",
        "outputId": "7c6505d1-2318-4a9f-eb73-c9dab699f870"
      },
      "outputs": [],
      "source": [
        "# Ruta al archivo CSV con los datos\n",
        "data_path = 'data.csv'\n",
        "\n",
        "# Carga los datos desde el archivo CSV en un DataFrame\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Muestra el DataFrame\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umNg4gfhgVpf"
      },
      "outputs": [],
      "source": [
        "# Selecciona las columnas de interés para el análisis\n",
        "data = df[['username', 'platform', 'text', 'num_interaction', 'candidate_name']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQgoYTyLzy5l"
      },
      "source": [
        "# 1.- Analizar los textos publicados por los 50 usuarios top de cada plataforma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cihH-ULHrIWj"
      },
      "source": [
        "## 1.1 Obtener los textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Rv9gQMIuIiYc",
        "outputId": "fd2ed6dc-eafa-4bcb-854b-57020b0456e3"
      },
      "outputs": [],
      "source": [
        "# Define cuántos usuarios principales por plataforma\n",
        "n = 1000\n",
        "\n",
        "# Suma las interacciones por usuario y plataforma\n",
        "top_users = data.groupby(['platform', 'username'])['num_interaction'].sum().reset_index()\n",
        "\n",
        "# Obtén el candidato más frecuente asociado a cada usuario\n",
        "users_candidate = data.groupby(['platform', 'username'])['candidate_name'].apply(lambda x: x.mode()[0]).reset_index()\n",
        "\n",
        "# Agrupa todos los textos publicados por cada usuario en una lista\n",
        "user_texts = data.groupby(['platform', 'username'])['text'].apply(list).reset_index()\n",
        "\n",
        "# Fusiona los DataFrames de interacciones, candidato y textos\n",
        "top_users = top_users.merge(users_candidate, on=['platform', 'username'])\n",
        "top_users = top_users.merge(user_texts, on=['platform', 'username'])\n",
        "\n",
        "# Ordena por plataforma y número de interacciones (descendente)\n",
        "top_users = top_users.sort_values(by=['platform', 'num_interaction'], ascending=[True, False])\n",
        "\n",
        "# Selecciona los primeros 'n' usuarios por plataforma\n",
        "top_users = top_users.groupby('platform').head(n)\n",
        "\n",
        "# Reinicia el índice tras el filtrado\n",
        "top_users.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Muestra el DataFrame resultante\n",
        "top_users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVFgxtXTeWKJ"
      },
      "outputs": [],
      "source": [
        "# Activa las barras de progreso en métodos pandas (apply, etc.)\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atvWKXu5FS-X",
        "outputId": "c3f0eefb-f9ed-4502-c54d-d558b6a01743"
      },
      "outputs": [],
      "source": [
        "# Descarga la lista de palabras vacías en español\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Crea el conjunto de stopwords en español y añade términos específicos del contexto\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "extra_stopwords = ['rt', 'https', 'creativecommonsorglicensesby40', 'http', 'creative',\n",
        "                   'atribución', 'commons', 'licencia', '_', '40', 'com', '__',\n",
        "                   'httpscreativecommonsorglicensesby40', 'httpaudionautixcom']\n",
        "stop_words.update(extra_stopwords)\n",
        "\n",
        "# Función para limpiar texto: pasa a minúsculas, elimina puntuación y filtra stopwords\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "# Aplica la limpieza a cada texto en la lista de publicaciones\n",
        "top_users['clean_text'] = top_users['text'].progress_apply(\n",
        "    lambda x: [clean_text(str(text)) for text in x]\n",
        ")\n",
        "# Concadena todos los textos originales por usuario en una sola cadena\n",
        "top_users['doc'] = top_users['text'].progress_apply(lambda x: ' '.join(x))\n",
        "# Concadena todos los textos ya limpiados por usuario en una sola cadena\n",
        "top_users['clean_doc'] = top_users['clean_text'].progress_apply(lambda x: ' '.join(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28PJ_IevRBKT",
        "outputId": "d61fae79-3fd7-41b5-ed62-483f868fb7ad"
      },
      "outputs": [],
      "source": [
        "# Diccionario para almacenar DataFrames por plataforma\n",
        "platform_dfs = {}\n",
        "\n",
        "# Itera sobre cada plataforma única en top_users\n",
        "for platform in top_users['platform'].unique():\n",
        "    # Filtra los usuarios de la plataforma actual y reinicia el índice\n",
        "    platform_df = top_users[top_users['platform'] == platform].reset_index(drop=True)\n",
        "\n",
        "    # Guarda el DataFrame filtrado en el diccionario\n",
        "    platform_dfs[platform] = platform_df\n",
        "\n",
        "    # Muestra las primeras filas para inspección rápida\n",
        "    display(platform_df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7gSqe1xexWO"
      },
      "outputs": [],
      "source": [
        "# Correción de errores en la obtención de datos, especificamente en la asignación de apoyo a candidato\n",
        "\n",
        "wrong_candidate_xochtil_users = ['Xóchitl Gálvez Ruiz', 'Xochitl2024', 'XochitlGalvez']\n",
        "for platform, platform_df in platform_dfs.items():\n",
        "    data = platform_df.copy()\n",
        "    for wrong_candidate in wrong_candidate_xochtil_users:\n",
        "        data.loc[data['username'] == wrong_candidate, 'candidate_name'] = 'Xóchitl Gálvez'\n",
        "    platform_dfs[platform] = data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BcpRVzwzqNm"
      },
      "source": [
        "# 2.- Generar representaciones vectoriales del discurso y agrupar usuarios por afinidad semántica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeaeLx80X2D0"
      },
      "source": [
        "## 2.1 Generar representaciones vectoriales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVPcjTWSXNSF",
        "outputId": "b98aa624-af0f-4eda-f16c-16906dafe087"
      },
      "outputs": [],
      "source": [
        "def train_doc2vec_supervised(\n",
        "    df: pd.DataFrame,\n",
        "    text_col: str,\n",
        "    label_col: str,\n",
        "    vocab_size: int = 20000,\n",
        "    maxlen: int = 200,\n",
        "    embed_dim: int = 128,\n",
        "    hidden_units: int = 64,\n",
        "    test_size: float = 0.2,\n",
        "    random_state: int = 123,\n",
        "    epochs: int = 10,\n",
        "    batch_size: int = 32\n",
        ") -> Tuple[\n",
        "    tf.keras.Model,      # modelo completo entrenado\n",
        "    Tokenizer,           # tokenizer ajustado\n",
        "    tf.keras.Model,      # encoder para extraer embeddings\n",
        "    np.ndarray           # embeddings de todos los documentos\n",
        "]:\n",
        "    # --- FIJAR SEMILLAS PARA DETERMINISMO ---\n",
        "    np.random.seed(random_state) # Semilla para NumPy\n",
        "    random.seed(random_state)    # Semilla para el módulo random de Python\n",
        "    tf.random.set_seed(random_state) # Semilla para TensorFlow\n",
        "\n",
        "    # 1) Codificar etiquetas de texto a vectores one-hot\n",
        "    le = LabelEncoder()\n",
        "    y_int = le.fit_transform(df[label_col])\n",
        "    y_cat = tf.keras.utils.to_categorical(y_int, num_classes=len(le.classes_))\n",
        "\n",
        "    # 2) Tokenización de textos y padding de las secuencias\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(df[text_col])\n",
        "    seqs = tokenizer.texts_to_sequences(df[text_col])\n",
        "    X = pad_sequences(seqs, maxlen=maxlen, padding='post')\n",
        "\n",
        "    # 3) División de datos en conjuntos de entrenamiento y validación\n",
        "    # random_state asegura que esta división sea determinista\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y_cat,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state, # Usando el random_state pasado a la función\n",
        "    )\n",
        "\n",
        "    # 4) Definición de la arquitectura: embedding → pooling → capa oculta → softmax\n",
        "    # Las inicializaciones de pesos serán deterministas gracias a tf.random.set_seed\n",
        "    inp = tf.keras.Input(shape=(maxlen,), name=\"input_doc\")\n",
        "    x   = tf.keras.layers.Embedding(\n",
        "              input_dim=vocab_size,\n",
        "              output_dim=embed_dim,\n",
        "              input_length=maxlen,\n",
        "              name=\"word_embedding\"\n",
        "          )(inp)\n",
        "    x   = tf.keras.layers.GlobalAveragePooling1D(name=\"doc_pooling\")(x)\n",
        "    x   = tf.keras.layers.Dense(hidden_units, activation='relu', name=\"dense_relu\")(x)\n",
        "    out = tf.keras.layers.Dense(y_cat.shape[1], activation='softmax', name=\"pred_label\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inp, outputs=out, name=\"Doc2Vec_Supervised\")\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 5) Entrenamiento del modelo\n",
        "    # El entrenamiento será determinista por las semillas fijadas\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 6) Construcción de encoder para obtener embeddings de documentos\n",
        "    encoder = tf.keras.Model(\n",
        "        inputs = model.input,\n",
        "        outputs = model.get_layer('doc_pooling').output,\n",
        "        name   = \"Doc2Vec_Encoder\"\n",
        "    )\n",
        "    doc_embeddings = encoder.predict(X, batch_size=batch_size)\n",
        "\n",
        "    return model, tokenizer, encoder, doc_embeddings\n",
        "\n",
        "\n",
        "# Para cada plataforma, entrena y guarda los embeddings en el DataFrame correspondiente\n",
        "for platform, data in platform_dfs.items():\n",
        "    df = data.copy()\n",
        "\n",
        "    # Puedes pasar un random_state diferente si quieres variabilidad entre plataformas,\n",
        "    # pero para total determinismo global, lo mantienes constante.\n",
        "    d2v_tf, tokenizer, encoder, doc_embeddings = train_doc2vec_supervised(\n",
        "        df,\n",
        "        text_col='clean_doc',\n",
        "        label_col='candidate_name',\n",
        "        vocab_size=20000,\n",
        "        maxlen=200,\n",
        "        embed_dim=128,\n",
        "        hidden_units=64,\n",
        "        test_size=0.2,\n",
        "        random_state=42, # Asegura que cada entrenamiento con esta semilla sea determinista\n",
        "        epochs=20,\n",
        "        batch_size=32\n",
        "    )\n",
        "    platform_dfs[platform] = df\n",
        "    platform_dfs[platform]['doc2vec_tf_embs'] = doc_embeddings.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHu9wGgkMxaf"
      },
      "outputs": [],
      "source": [
        "# Función para generar vectores TF-IDF a partir de los textos limpios\n",
        "def create_tf_idf_vectors(df, ngram_range=(1, 1), max_features=5000):\n",
        "    # Inicializa el vectorizador con rango de n-gramas y número máximo de características\n",
        "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    # Ajusta el modelo y transforma los documentos en una matriz dispersa de TF-IDF\n",
        "    vectors = vectorizer.fit_transform(df['clean_doc'])\n",
        "    return vectors\n",
        "\n",
        "# Aplica la creación de vectores TF-IDF para cada plataforma\n",
        "for platform, data in platform_dfs.items():\n",
        "    temp_df = data.copy()\n",
        "    # Genera vectores TF-IDF (ngramas de 1 a 2, hasta 3000 características) y almacénalos como listas\n",
        "    temp_df['tf-idf_vectors'] = create_tf_idf_vectors(\n",
        "        temp_df,\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=3000\n",
        "    ).toarray().tolist()\n",
        "    platform_dfs[platform] = temp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEl60fMnUYcE",
        "outputId": "f859d61b-bba0-4847-c286-48e2e4340110"
      },
      "outputs": [],
      "source": [
        "def find_best_num_topics(texts, max_topics=15):\n",
        "    # 1) Vectoriza los documentos en una matriz término-documento\n",
        "    vectorizer = CountVectorizer()\n",
        "    doc_term_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "    # 2) Prueba distintos números de temas y guarda el error de reconstrucción\n",
        "    reconstruction_errors = []\n",
        "    topic_range = range(2, max_topics + 1)\n",
        "    for n_topics in topic_range:\n",
        "        nmf_model = NMF(\n",
        "            n_components=n_topics,\n",
        "            init='nndsvd',          # inicialización recomendada para texto\n",
        "            random_state=42,\n",
        "            max_iter=200\n",
        "        )\n",
        "        nmf_model.fit(doc_term_matrix)\n",
        "        # reconstruction_err_ mide el error de reconstrucción tras el fit\n",
        "        reconstruction_errors.append(nmf_model.reconstruction_err_)\n",
        "\n",
        "    # 3) Retorna el número de temas que minimiza el error\n",
        "    best_n_topics = topic_range[np.argmin(reconstruction_errors)]\n",
        "    return best_n_topics\n",
        "\n",
        "def create_topic_model_vectors(df, num_topics):\n",
        "    # 1) Vectoriza los documentos nuevamente\n",
        "    vectorizer = CountVectorizer()\n",
        "    doc_term_matrix = vectorizer.fit_transform(df['clean_doc'])\n",
        "\n",
        "    # 2) Ajusta NMF y obtiene las representaciones W (n_docs × num_topics)\n",
        "    nmf_model = NMF(\n",
        "        n_components=num_topics,\n",
        "        init='nndsvd',\n",
        "        random_state=42,\n",
        "        max_iter=200\n",
        "    )\n",
        "    topic_vectors = nmf_model.fit_transform(doc_term_matrix)\n",
        "\n",
        "    # 3) Construye etiquetas de tema con las top palabras de cada componente H\n",
        "    topic_labels = {}\n",
        "    top_n = 10\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    for topic_idx, component in enumerate(nmf_model.components_):\n",
        "        top_words = [\n",
        "            feature_names[i]\n",
        "            for i in component.argsort()[:-top_n - 1:-1]\n",
        "        ]\n",
        "        topic_labels[topic_idx] = \", \".join(top_words)\n",
        "\n",
        "    return topic_vectors, topic_labels\n",
        "\n",
        "# --- Aplicación del modelado de temas a cada plataforma ---\n",
        "topic_label_dicts = {}\n",
        "\n",
        "for platform, data in platform_dfs.items():\n",
        "    temp_df = data.copy()\n",
        "    # Determina el mejor número de temas para esta plataforma\n",
        "    best_n = find_best_num_topics(temp_df['clean_doc'])\n",
        "    # Crea los vectores de temas y las etiquetas correspondientes\n",
        "    topic_vectors, topic_labels = create_topic_model_vectors(\n",
        "        temp_df, num_topics=best_n\n",
        "    )\n",
        "    # Almacena los vectores de temas en el DataFrame\n",
        "    temp_df['topic_vectors'] = topic_vectors.tolist()\n",
        "    platform_dfs[platform] = temp_df\n",
        "    # Guarda las etiquetas de cada tema para referencia\n",
        "    topic_label_dicts[platform] = topic_labels\n",
        "\n",
        "    # Imprime un resumen de los temas detectados\n",
        "    print(f\"Temas por plataforma {platform}:\")\n",
        "    for topic_idx, labels in topic_labels.items():\n",
        "        print(f\"\\tTema {topic_idx + 1}: {labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKglQv9HBKX-"
      },
      "outputs": [],
      "source": [
        "# Define las columnas de vectores de características a combinar y asigna pesos iguales a cada una\n",
        "vector_columns = ['tf-idf_vectors', 'doc2vec_tf_embs', 'topic_vectors']\n",
        "column_weights = [1/len(vector_columns)] * len(vector_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSS5NJWTxZ-Z",
        "outputId": "9982f105-26e5-493d-c184-2139ebc2166e"
      },
      "outputs": [],
      "source": [
        "# Función para normalizar vectores de forma segura (evita división por cero)\n",
        "def safe_normalize(x):\n",
        "    norm = np.linalg.norm(x)\n",
        "    if norm == 0:\n",
        "        return x\n",
        "    return x / norm\n",
        "\n",
        "# Itera sobre cada plataforma y aplica la normalización a cada columna de vectores\n",
        "for platform, data in platform_dfs.items():\n",
        "    temp_df = data.copy()\n",
        "    for col in tqdm(vector_columns, desc=f\"{platform} - columnas\", leave=False):\n",
        "        # Normaliza cada vector en la columna y vuelve a lista\n",
        "        temp_df[col] = temp_df[col].apply(lambda x: safe_normalize(np.array(x)).tolist())\n",
        "    platform_dfs[platform] = temp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SETb63HMftNJ"
      },
      "source": [
        "## 2.2 Agrupar usuarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UrcgQhz-PZ85",
        "outputId": "cd748183-c54f-431f-c3da-c676791144af"
      },
      "outputs": [],
      "source": [
        "# Columnas que contienen diferentes representaciones vectoriales a evaluar\n",
        "tvector_columns = ['tf-idf_vectors', 'doc2vec_tf_embs', 'topic_vectors']\n",
        "\n",
        "# Configuración de métricas de evaluación y modelos con sus rangos de hiperparámetros\n",
        "METRICS = {\n",
        "    'calinski_harabasz': calinski_harabasz_score\n",
        "}\n",
        "MODELS = {\n",
        "    'KMeans': {\n",
        "        'model': KMeans,\n",
        "        'params': {\n",
        "            'n_clusters': list(range(2, 10)),\n",
        "            'init': ['k-means++', 'random'],\n",
        "            'random_state': [42]\n",
        "        }\n",
        "    },\n",
        "    'Agglomerative': {\n",
        "        'model': AgglomerativeClustering,\n",
        "        'params': {\n",
        "            'n_clusters': list(range(2, 10)),\n",
        "            'linkage': ['ward', 'complete', 'average', 'single']\n",
        "        }\n",
        "    },\n",
        "    'SpectralClustering': {\n",
        "        'model': SpectralClustering,\n",
        "        'params': {\n",
        "            'n_clusters': list(range(2, 10)),\n",
        "            'affinity': ['nearest_neighbors', 'rbf'],\n",
        "            'random_state': [42],\n",
        "            'assign_labels': ['kmeans', 'discretize']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def evaluate_clustering(X, labels, metrics):\n",
        "    results = {}\n",
        "    # Algunos algoritmos pueden generar etiquetas -1 para outliers, evitar errores:\n",
        "    if len(set(labels)) == 1 or -1 in labels:\n",
        "        # Cluster único o etiquetas inválidas, métrica no computable o mínima\n",
        "        for metric_name in metrics.keys():\n",
        "            results[metric_name] = -np.inf\n",
        "        return results\n",
        "\n",
        "    for metric_name, metric_func in metrics.items():\n",
        "        results[metric_name] = metric_func(X, labels)\n",
        "    return results\n",
        "\n",
        "def optimize_hyperparameters(X, metrics, model_name, model, params, n_iter=20):\n",
        "    sampler = ParameterSampler(params, n_iter=n_iter, random_state=42)\n",
        "    best_score = float('-inf')\n",
        "    best_params = None\n",
        "    best_labels = None\n",
        "\n",
        "    for param_set in tqdm(sampler, desc=f\"{model_name} - parametros\", leave=False):\n",
        "        try:\n",
        "            model_instance = clone(model(**param_set))\n",
        "            model_instance.fit(X)\n",
        "            labels = model_instance.labels_ if hasattr(model_instance, \"labels_\") else model_instance.fit_predict(X)\n",
        "            evaluation = evaluate_clustering(X, labels, metrics)\n",
        "            score = list(evaluation.values())[0]\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = param_set\n",
        "                best_labels = labels\n",
        "        except Exception as e:\n",
        "            # En caso de error en parámetros, saltar\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        'best_params': best_params,\n",
        "        'best_labels': best_labels,\n",
        "        'best_score': best_score,\n",
        "        'model_name': model_name\n",
        "    }\n",
        "\n",
        "def get_best_model(X, metrics, models, n_iter=20):\n",
        "    results = []\n",
        "    for model_name, model_info in models.items():\n",
        "        res = optimize_hyperparameters(X, metrics, model_name,\n",
        "                                       model_info['model'], model_info['params'], n_iter)\n",
        "        results.append(res)\n",
        "    return max(results, key=lambda x: x['best_score']), results\n",
        "\n",
        "def align_labels_hungarian(reference_labels, target_labels):\n",
        "    ref_clusters = np.unique(reference_labels)\n",
        "    target_clusters = np.unique(target_labels)\n",
        "    n_ref = len(ref_clusters)\n",
        "    n_target = len(target_clusters)\n",
        "    cost_matrix = np.zeros((n_ref, n_target))\n",
        "\n",
        "    for i, ref_cluster in enumerate(ref_clusters):\n",
        "        for j, target_cluster in enumerate(target_clusters):\n",
        "            intersection = np.sum((reference_labels == ref_cluster) &\n",
        "                                  (target_labels == target_cluster))\n",
        "            cost_matrix[i, j] = -intersection\n",
        "\n",
        "    row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "    label_mapping = {}\n",
        "    for i, j in zip(row_indices, col_indices):\n",
        "        if j < len(target_clusters):\n",
        "            label_mapping[target_clusters[j]] = ref_clusters[i]\n",
        "\n",
        "    aligned_labels = np.copy(target_labels)\n",
        "    for old_label, new_label in label_mapping.items():\n",
        "        aligned_labels[target_labels == old_label] = new_label\n",
        "\n",
        "    return aligned_labels\n",
        "\n",
        "def late_fusion_weighted_voting(labels_dict, scores_dict):\n",
        "    views = list(labels_dict.keys())\n",
        "    n_samples = len(list(labels_dict.values())[0])\n",
        "\n",
        "    total_score = sum(scores_dict.values())\n",
        "    weights = {view: score / total_score for view, score in scores_dict.items()}\n",
        "\n",
        "    reference_view = views[0]\n",
        "    reference_labels = labels_dict[reference_view]\n",
        "    aligned_labels = {reference_view: reference_labels}\n",
        "\n",
        "    for view in views[1:]:\n",
        "        aligned_labels[view] = align_labels_hungarian(\n",
        "            reference_labels, labels_dict[view]\n",
        "        )\n",
        "\n",
        "    final_labels = np.zeros(n_samples, dtype=int)\n",
        "    voting_confidence = np.zeros(n_samples)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        cluster_votes = {}\n",
        "        for view, labels in aligned_labels.items():\n",
        "            cluster_votes[labels[i]] = cluster_votes.get(labels[i], 0) + weights[view]\n",
        "        best_cluster, best_weight = max(cluster_votes.items(), key=lambda x: x[1])\n",
        "        final_labels[i] = best_cluster\n",
        "        voting_confidence[i] = best_weight\n",
        "\n",
        "    voting_info = {\n",
        "        'weights': weights,\n",
        "        'aligned_labels': aligned_labels,\n",
        "        'confidence': voting_confidence\n",
        "    }\n",
        "    return final_labels, voting_info\n",
        "\n",
        "def get_labels_with_late_fusion(views, view_names, metrics, models, n_iter=20):\n",
        "    results = {}\n",
        "    for X, name in zip(views, view_names):\n",
        "        best_model_result, all_results = get_best_model(X, metrics, models, n_iter)\n",
        "        results[name] = {\n",
        "            'best': best_model_result,\n",
        "            'all': all_results\n",
        "        }\n",
        "\n",
        "    best_global_result = max([res['best'] for res in results.values()], key=lambda x: x['best_score'])\n",
        "    model_name = best_global_result['model_name']\n",
        "    best_params = best_global_result['best_params']\n",
        "\n",
        "    labels_per_view = {}\n",
        "    scores_per_view = {}\n",
        "\n",
        "    for i, (view_name, res) in enumerate(results.items()):\n",
        "        model_instance = clone(models[model_name]['model'](**best_params))\n",
        "        model_instance.fit(views[i])\n",
        "        labels = model_instance.labels_ if hasattr(model_instance, \"labels_\") else model_instance.fit_predict(views[i])\n",
        "        labels_per_view[view_name] = labels\n",
        "        evaluation = evaluate_clustering(views[i], labels, metrics)\n",
        "        scores_per_view[view_name] = list(evaluation.values())[0]\n",
        "\n",
        "    fusion_labels, fusion_info = late_fusion_weighted_voting(\n",
        "        labels_per_view, scores_per_view\n",
        "    )\n",
        "\n",
        "    return fusion_labels, fusion_info, results\n",
        "\n",
        "def print_clusters(labels, names):\n",
        "    for c in np.unique(labels):\n",
        "        members = names[labels == c]\n",
        "        print(f\"Cluster {c}: {', '.join(members)}\")\n",
        "\n",
        "# Diccionario para nombres más amigables de los vectores\n",
        "friendly_names = {\n",
        "    'tf-idf_vectors': 'TF-IDF',\n",
        "    'doc2vec_tf_embs': 'Embeddings de documentos',\n",
        "    'topic_vectors': 'Vectores de tópicos'\n",
        "}\n",
        "\n",
        "def plot_model_comparison(all_results, vector_name, ax):\n",
        "    data = []\n",
        "    for res in all_results:\n",
        "        score = res['best_score']\n",
        "        params = res['best_params']\n",
        "        model = res['model_name']\n",
        "        data.append({\n",
        "            'Modelo': model,\n",
        "            'Score': score,\n",
        "        })\n",
        "\n",
        "    df_plot = pd.DataFrame(data)\n",
        "    sns.barplot(data=df_plot, x='Modelo', y='Score', dodge=False, palette='viridis', ax=ax)\n",
        "    # Usar nombre amigable para el título\n",
        "    ax.set_title(f\"Clustering por {friendly_names.get(vector_name, vector_name)}\", fontsize=12)\n",
        "    ax.set_ylabel('Calinski-Harabasz Score', fontsize=10)\n",
        "    ax.set_xlabel('')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    ax.get_legend().remove() if ax.get_legend() else None\n",
        "\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.2f', label_type='edge', padding=1)\n",
        "\n",
        "    ymax = ax.get_ylim()[1]\n",
        "    ax.set_ylim(top=ymax * 1.1)\n",
        "\n",
        "\n",
        "# --- EJEMPLO DE USO para cada plataforma ---\n",
        "weights_per_platform = {}\n",
        "\n",
        "for platform, data in platform_dfs.items():\n",
        "    print(f\"Plataforma: {platform}\")\n",
        "    df = data.copy()\n",
        "    X_views = [np.array(df[col].tolist()) for col in tvector_columns]\n",
        "\n",
        "    fusion_labels, fusion_info, all_results_dict = get_labels_with_late_fusion(\n",
        "        X_views, tvector_columns, METRICS, MODELS, n_iter=16\n",
        "    )\n",
        "    weights_per_platform[platform] = fusion_info['weights']\n",
        "    print_clusters(fusion_labels, df['username'])\n",
        "    df['voted_cluster'] = fusion_labels\n",
        "    platform_dfs[platform] = df\n",
        "\n",
        "    # Crear figura con subplots lado a lado, uno por vector\n",
        "    fig, axs = plt.subplots(1, len(tvector_columns), figsize=(4 * len(tvector_columns), 5), squeeze=False)\n",
        "\n",
        "    for i, view_name in enumerate(tvector_columns):\n",
        "        all_results = all_results_dict[view_name]['all']\n",
        "        plot_model_comparison(all_results, view_name, axs[0, i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SmOlT0QONsi"
      },
      "outputs": [],
      "source": [
        "# Este diccionario almacenará los nombres de los clusters para cada plataforma.\n",
        "cluster_names = {\n",
        "    'Facebook': {\n",
        "        0: 'Partidarios de Xóchitl Gálvez y Oposición',\n",
        "        1: 'Noticias y Figuras Políticas Diversas',\n",
        "        2: 'Partidarios de Morena y la 4T',\n",
        "        3: 'Partidarios de Movimiento Ciudadano',\n",
        "        4: 'Contenido de Memes y Entretenimiento',\n",
        "    },\n",
        "    'Instagram': {\n",
        "        0: 'Medios Globales y Nacionales con Figuras Políticas Diversas',\n",
        "        1: 'Figuras Políticas, Medios y Contenido Diverso',\n",
        "        2: 'Movimiento Ciudadano (Políticos y Afines)',\n",
        "        3: 'Líderes de Oposición y Partidarios de Xóchitl Gálvez',\n",
        "        4: 'Partidarios de Claudia Sheinbaum y la 4T (Morena y Afines)',\n",
        "        5: 'Partidos Políticos de Oposición (PRI, PAN, PRD y Afines)',\n",
        "    },\n",
        "    'X': {\n",
        "        0: 'Líderes Políticos y Medios Globales',\n",
        "        1: 'Actores Clave del Escenario Político Mexicano',\n",
        "        2: 'Movimiento Ciudadano y Críticos Políticos',\n",
        "        3: 'Partidarios de Oposición y Críticos de la 4T (Xóchitl Gálvez)',\n",
        "    },\n",
        "    'Youtube': {\n",
        "        0: 'Canales de Noticias y Opinión Variada',\n",
        "        1: 'Medios de Comunicación y Análisis Político',\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDniE7G6Q13w"
      },
      "outputs": [],
      "source": [
        "for platform, data in platform_dfs.items():\n",
        "    df = data.copy()\n",
        "    df['cluster_name'] = df['voted_cluster'].map(cluster_names[platform])\n",
        "    platform_dfs[platform] = df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DFkxIAHYIte"
      },
      "source": [
        "# 3 Mapear la estructura de la red entre ellos y medir métricas de cohesión y polarización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTzTJpGjGxEu"
      },
      "source": [
        "## 3.1 Mapear la estructura de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BT4zUG7MxQF"
      },
      "outputs": [],
      "source": [
        "def create_similarity_matrix(df, vector_column, similarity_function=cosine_similarity):\n",
        "    # Convierte la columna de vectores en una matriz NumPy\n",
        "    vectors = np.array(df[vector_column].to_list())\n",
        "\n",
        "    # Calcula la similitud entre todos los pares de vectores\n",
        "    similarity_matrix = similarity_function(vectors)\n",
        "\n",
        "    # Anula la similitud de cada elemento consigo mismo\n",
        "    for i in range(len(similarity_matrix)):\n",
        "        similarity_matrix[i][i] = 0\n",
        "\n",
        "    # Devuelve un DataFrame indexado por username para facilitar la lectura\n",
        "    similarity_df = pd.DataFrame(\n",
        "        similarity_matrix,\n",
        "        index=df['username'],\n",
        "        columns=df['username']\n",
        "    )\n",
        "    return similarity_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwtiBGuZPivp"
      },
      "outputs": [],
      "source": [
        "def combine_matrices(matrices, weights):\n",
        "    # Convierte las matrices en arrays de NumPy\n",
        "    arrays = [np.array(matrix) for matrix in matrices]\n",
        "    # Inicializa un array vacío con la misma forma\n",
        "    combined_array = np.zeros_like(arrays[0])\n",
        "\n",
        "    # Suma ponderada de cada matriz\n",
        "    for array, weight in zip(arrays, weights):\n",
        "        combined_array += array * weight\n",
        "\n",
        "    # Convierte de nuevo a lista y crea un DataFrame con índices y columnas\n",
        "    combined = combined_array.tolist()\n",
        "    df = pd.DataFrame(combined, index=matrices[0].index, columns=matrices[0].columns)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7zrV5tR4Ytt"
      },
      "outputs": [],
      "source": [
        "similarity_matrices = {}\n",
        "\n",
        "# Para cada plataforma, crea y combina sus matrices de similitud usando los pesos definidos\n",
        "for platform, data in platform_dfs.items():\n",
        "    # Obtener el dataframe\n",
        "    df = data.copy()\n",
        "\n",
        "    # Obtener pesos y columnas\n",
        "    vector_cols = list(weights_per_platform[platform].keys())\n",
        "    weights = list(weights_per_platform[platform].values())\n",
        "\n",
        "    # Definir matrices\n",
        "    sim_matrices = {}\n",
        "    for col in vector_cols:\n",
        "        sim_matrices[col] = create_similarity_matrix(df, col)\n",
        "\n",
        "    # Juntar matrices y asignar\n",
        "    combined = combine_matrices(list(sim_matrices.values()), weights)\n",
        "    similarity_matrices[platform] = {\n",
        "        'combined': combined,\n",
        "        'individual': sim_matrices\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXQ7Ctzkaobq",
        "outputId": "f8a949c0-d52d-4365-a507-b04104241ea1"
      },
      "outputs": [],
      "source": [
        "def create_influence_network(df, similarity_matrix):\n",
        "    # Inicializa un grafo dirigido y una lista para acumular pesos de influencia\n",
        "    G = nx.DiGraph()\n",
        "    influence_weights = []\n",
        "\n",
        "    # Añade cada usuario como nodo con sus atributos relevantes\n",
        "    for user in similarity_matrix.index:\n",
        "        row = df[df['username'] == user].iloc[0]\n",
        "        candidate = row['candidate_name']\n",
        "        # Asigna etiqueta numérica al candidato\n",
        "        if candidate == 'Claudia Sheinbaum':\n",
        "            candidate_label = 1\n",
        "        elif candidate == 'Jorge Álvarez Máynez':\n",
        "            candidate_label = 3\n",
        "        else:\n",
        "            candidate_label = 0\n",
        "\n",
        "        G.add_node(\n",
        "            user,\n",
        "            #doc=row['doc'],\n",
        "            candidate=candidate,\n",
        "            candidate_label=candidate_label,\n",
        "            num_interaction=row['num_interaction'],\n",
        "            tf_idf_vector=row['tf-idf_vectors'],\n",
        "            doc2vec_vector=row['doc2vec_tf_embs'],\n",
        "            topic_vector=row['topic_vectors'],\n",
        "            cluster=row['voted_cluster'],\n",
        "            cluster_name=row['cluster_name']\n",
        "        )\n",
        "\n",
        "    # Calcula todos los posibles pesos de similtud\n",
        "    for u1 in similarity_matrix.index:\n",
        "        for u2 in similarity_matrix.columns:\n",
        "            sim = similarity_matrix.loc[u1, u2]\n",
        "            infl = sim\n",
        "            influence_weights.append(infl)\n",
        "\n",
        "    # Determina el umbral como el percentil n de los pesos\n",
        "    threshold = np.percentile(influence_weights, 90)\n",
        "\n",
        "    # Añade aristas sólo si el peso supera el umbral\n",
        "    for u1 in similarity_matrix.index:\n",
        "        for u2 in similarity_matrix.columns:\n",
        "            sim = similarity_matrix.loc[u1, u2]\n",
        "            weight = sim\n",
        "            if weight > threshold:\n",
        "                G.add_edge(u1, u2, weight=weight)\n",
        "\n",
        "    return G\n",
        "\n",
        "# Construcción de la red de influencia para cada plataforma\n",
        "platform_networks = {}\n",
        "for platform, data in platform_dfs.items():\n",
        "    print(f\"---- Creando Red para Plataforma: {platform} ----\")\n",
        "    sim_matrix = similarity_matrices[platform]['combined']\n",
        "    G = create_influence_network(data.copy(), sim_matrix)\n",
        "    platform_networks[platform] = G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGxVfwor-OUM"
      },
      "outputs": [],
      "source": [
        "def get_individual_network(graph, individual_node):\n",
        "    # Reúne vecinos de aristas entrantes y salientes, usando el peso máximo si existen ambas\n",
        "    neighbors = {}\n",
        "    # Obtiene posibles vecinos combinando predecesores y sucesores\n",
        "    potential_neigh = set(graph.predecessors(individual_node)).union(set(graph.successors(individual_node)))\n",
        "    for neighbor in potential_neigh:\n",
        "        # Peso de la arista saliente (individual -> vecino)\n",
        "        weight_out = graph[individual_node][neighbor]['weight'] if graph.has_edge(individual_node, neighbor) else 0\n",
        "        # Peso de la arista entrante (vecino -> individual)\n",
        "        weight_in = graph[neighbor][individual_node]['weight'] if graph.has_edge(neighbor, individual_node) else 0\n",
        "        # Selecciona el mayor de los dos\n",
        "        neighbors[neighbor] = max(weight_in, weight_out)\n",
        "    # Ordena vecinos por peso descendente y construye la lista de nodos del subgrafo\n",
        "    selected_neighbors = sorted(neighbors, key=neighbors.get, reverse=True)\n",
        "    sub_nodes = selected_neighbors + [individual_node]\n",
        "    # Devuelve el subgrafo inducido por el individual y sus vecinos\n",
        "    return graph.subgraph(sub_nodes).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDrybxXof1sE"
      },
      "outputs": [],
      "source": [
        "politicians = ['Claudia Sheinbaum', 'Claudia Sheinbaum Pardo', 'Xóchitl Gálvez Ruiz', 'Xochitl2024', 'Jorge Álvarez Máynez', 'Mario Delgado Carrillo', 'Mario Delgado', 'Miguel Torruco Garza', 'Samuel García', 'Kenia López Rabadán', 'Rocío Nahle', 'Tatiana Clouthier', 'Lilia Aguilar', 'Renán Barrera', 'Marcelo Ebrard', 'Alejandro Moreno']\n",
        "\n",
        "media = ['El Universal Online', 'Conexión Mx', 'Sin Censura TV', 'EL FINANCIERO', 'Campaigns and Elections Mexico', 'Radio Fórmula', 'MILENIO', 'sdpnoticias', 'Político MX', 'LA OCTAVA', 'Reporte Índigo', 'Imagen Noticias', 'El Heraldo de México', 'La Jornada', 'Once Noticias']\n",
        "\n",
        "political_parties = ['Partido del Trabajo México', 'Partido Morena', 'PRI', 'Movimiento Ciudadano', 'Partido Acción Nacional', 'PartidoMorenaMx']\n",
        "\n",
        "\n",
        "# Agrupamos las listas bajo un mismo diccionario para iterar fácilmente\n",
        "categories = {\n",
        "    'politicos': politicians,\n",
        "    'medios': media,\n",
        "    'partidos': political_parties,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aq5BBzRx7Fj",
        "outputId": "f5680592-1ec0-4ffd-a5e6-db8d8fb2777e"
      },
      "outputs": [],
      "source": [
        "# Diccionario para almacenar las ego-redes por categoría y usuario\n",
        "individual_networks = {}\n",
        "\n",
        "# Iteramos sobre cada plataforma y su red correspondiente\n",
        "for platform, G in platform_networks.items():\n",
        "    print(f\"Creando redes individuales para la plataforma: {platform}\")\n",
        "    individual_networks[platform] = {}\n",
        "\n",
        "    # Iteramos sobre cada categoría de usuarios relevantes\n",
        "    for category_name, user_list in categories.items():\n",
        "        individual_networks[platform][category_name] = {}\n",
        "        # Iteramos sobre cada usuario dentro de la categoría\n",
        "        for user in user_list:\n",
        "            # Verificamos si el usuario existe en la red actual\n",
        "            if user in G:\n",
        "                # Obtenemos la ego-red para el usuario\n",
        "                ego_graph = get_individual_network(G, user)\n",
        "                # Almacenamos la ego-red en el diccionario\n",
        "                individual_networks[platform][category_name][user] = ego_graph\n",
        "                print(f\"      Ego-red de {user} creada con {ego_graph.number_of_nodes()} nodos y {ego_graph.number_of_edges()} aristas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lkNRVCBDyFF"
      },
      "outputs": [],
      "source": [
        "def plot_network(graph, layout, title=\"Network\", highlight_node=None):\n",
        "    cluster_names_raw = list(set(nx.get_node_attributes(graph, 'cluster_name').values()))\n",
        "    cluster_names = sorted([str(name) if isinstance(name, (str, int, float)) else 'Unknown' for name in cluster_names_raw if isinstance(name, (str, int, float))])\n",
        "\n",
        "    neon_colors = [\n",
        "        '#00FFFF', '#FF0080', '#00FF41', '#FF4000', '#8000FF', '#FFFF00',\n",
        "        '#FF0040', '#40FF00', '#0080FF', '#FF8000', '#80FF00', '#FF00FF'\n",
        "    ]\n",
        "\n",
        "    cluster_plotly_colors = {\n",
        "        name: neon_colors[i % len(neon_colors)]\n",
        "        for i, name in enumerate(cluster_names)\n",
        "    }\n",
        "\n",
        "    edge_x, edge_y = [], []\n",
        "    for edge in graph.edges():\n",
        "        x0, y0 = layout[edge[0]]\n",
        "        x1, y1 = layout[edge[1]]\n",
        "        edge_x += [x0, x1, None]\n",
        "        edge_y += [y0, y1, None]\n",
        "\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=1, color='rgba(100, 100, 255, 0.3)'),\n",
        "        hoverinfo='none', mode='lines',\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    node_x, node_y = [], []\n",
        "    node_colors, node_sizes = [], []\n",
        "    halo_sizes, outer_halo_sizes, node_text = [], [], []\n",
        "    symbols = []\n",
        "\n",
        "    for node, data in graph.nodes(data=True):\n",
        "        x, y = layout[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "\n",
        "        cluster_name = data.get('cluster_name', 'Unknown')\n",
        "        base_color = cluster_plotly_colors.get(str(cluster_name), '#FFFFFF')\n",
        "\n",
        "        degree = graph.degree(node)\n",
        "        base_size = 4\n",
        "        log_scale = np.emath.logn(64, degree + 1) * 3\n",
        "        node_size = base_size + log_scale\n",
        "\n",
        "        if highlight_node is not None and str(node) == str(highlight_node):\n",
        "            node_colors.append('#FFD700')\n",
        "            node_sizes.append(node_size * 1.8)\n",
        "            halo_sizes.append(node_size * 4)\n",
        "            outer_halo_sizes.append(node_size * 6)\n",
        "            symbols.append('star')\n",
        "        else:\n",
        "            node_colors.append(base_color)\n",
        "            node_sizes.append(node_size)\n",
        "            halo_sizes.append(node_size * 2.5)\n",
        "            outer_halo_sizes.append(node_size * 3.5)\n",
        "            symbols.append('circle')\n",
        "\n",
        "        hover_info = f\"<b>User: {node}</b><br>\"\n",
        "        hover_info += f\"<span style='color: #00FFFF'>Connections:</span> {degree}<br>\"\n",
        "        for key, value in data.items():\n",
        "            if not isinstance(value, (list, dict)) and len(str(value)) < 100:\n",
        "                hover_info += f\"<span style='color: #00FFFF'>{key.capitalize()}:</span> {value}<br>\"\n",
        "        node_text.append(hover_info)\n",
        "\n",
        "    halo_trace = go.Scatter(\n",
        "        x=node_x, y=node_y, mode='markers',\n",
        "        hoverinfo='none',\n",
        "        marker=dict(\n",
        "            color=node_colors,\n",
        "            size=halo_sizes,\n",
        "            opacity=0.15, line=dict(width=0)\n",
        "        ),\n",
        "        showlegend=False  # ocultar de leyenda\n",
        "    )\n",
        "\n",
        "    outer_halo_trace = go.Scatter(\n",
        "        x=node_x, y=node_y, mode='markers',\n",
        "        hoverinfo='none',\n",
        "        marker=dict(\n",
        "            color=node_colors,\n",
        "            size=outer_halo_sizes,\n",
        "            opacity=0.08, line=dict(width=0)\n",
        "        ),\n",
        "        showlegend=False  # ocultar de leyenda\n",
        "    )\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers',\n",
        "        hoverinfo='text',\n",
        "        text=node_text,\n",
        "        marker=dict(\n",
        "            showscale=False,\n",
        "            color=node_colors,\n",
        "            size=node_sizes,\n",
        "            line=dict(width=0),\n",
        "            opacity=0.7,\n",
        "            symbol=symbols\n",
        "        ),\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    # Leyenda: colores de los clusters\n",
        "    legend_traces = []\n",
        "    for name, color in cluster_plotly_colors.items():\n",
        "        trace = go.Scatter(\n",
        "            x=[None], y=[None],\n",
        "            mode='markers',\n",
        "            marker=dict(size=10, color=color),\n",
        "            legendgroup=str(name),\n",
        "            showlegend=True,\n",
        "            name=f'{name}'\n",
        "        )\n",
        "        legend_traces.append(trace)\n",
        "\n",
        "    fig = go.Figure(data=legend_traces + [outer_halo_trace, halo_trace, edge_trace, node_trace],\n",
        "                 layout=go.Layout(\n",
        "                    title={\n",
        "                        'text': f'<span style=\"color: #000000; font-size: 24px;\"><b>{title}</b></span>',\n",
        "                        'x': 0.5, 'xanchor': 'center'\n",
        "                    },\n",
        "                    showlegend=True,\n",
        "                    hovermode='closest',\n",
        "                    plot_bgcolor='#040238',\n",
        "                    paper_bgcolor='rgb(246, 248, 250)',\n",
        "                    font=dict(color='#000000', family='Arial Black'),\n",
        "                    margin=dict(b=20, l=5, r=5, t=60),\n",
        "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, showline=False),\n",
        "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, showline=False),\n",
        "                    annotations=[]\n",
        "                ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        hoverlabel=dict(\n",
        "            bgcolor=\"rgba(0, 0, 0, 0.8)\",\n",
        "            bordercolor=\"cyan\",\n",
        "            font_size=12,\n",
        "            font_family=\"Arial\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psaWA6HzGWjk"
      },
      "outputs": [],
      "source": [
        "def plot_node_density(graph, layout, title=\"Node Density Plot\", bandwidth=None, grid_size=100):\n",
        "    # Extrae coordenadas de todos los nodos\n",
        "    coords = np.array([layout[n] for n in graph.nodes()])\n",
        "    x, y = coords[:, 0], coords[:, 1]\n",
        "\n",
        "    # Estima la densidad con KDE gaussiano\n",
        "    kde = gaussian_kde(np.vstack([x, y]), bw_method=bandwidth)\n",
        "\n",
        "    # Crea una malla regular sobre el rango de datos\n",
        "    xi = np.linspace(x.min(), x.max(), grid_size)\n",
        "    yi = np.linspace(y.min(), y.max(), grid_size)\n",
        "    xx, yy = np.meshgrid(xi, yi)\n",
        "\n",
        "    # Evalúa la densidad en cada punto de la malla\n",
        "    zz = kde(np.vstack([xx.ravel(), yy.ravel()])).reshape(grid_size, grid_size)\n",
        "\n",
        "    # Aplica la transformación logarítmica\n",
        "    zz_log = np.log2(zz + 1)  # log(1 + x) para evitar -inf en ceros\n",
        "\n",
        "    # Crea el trazo de contorno de densidad con escala logarítmica\n",
        "    density_trace = go.Contour(\n",
        "        x=xi, y=yi, z=zz_log,\n",
        "        colorscale='Hot',\n",
        "        contours=dict(\n",
        "            coloring='heatmap',\n",
        "            showlabels=False\n",
        "        ),\n",
        "        hoverinfo='skip',\n",
        "        showscale=True,\n",
        "        colorbar=dict(title='log2(1 + Densidad)')\n",
        "    )\n",
        "\n",
        "    # Monta la figura\n",
        "    fig = go.Figure(data=[density_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title=f'<b>{title}</b>',\n",
        "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        margin=dict(b=20, l=5, r=5, t=40)\n",
        "                    ))\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFQdBpH2EBqq"
      },
      "outputs": [],
      "source": [
        "def plot_arc_diagram(source, target, weights, title):\n",
        "    all_nodes = sorted(set(source + target))\n",
        "    node_indices = {name: idx for idx, name in enumerate(all_nodes)}\n",
        "\n",
        "    x = list(range(len(all_nodes)))\n",
        "    y = [0] * len(all_nodes)\n",
        "\n",
        "    cmap = plt.colormaps['viridis']\n",
        "    node_colors = [mcolors.rgb2hex(cmap(i / (len(all_nodes)-1))) for i in range(len(all_nodes))]\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=22,\n",
        "            color=node_colors,\n",
        "            line=dict(width=1.5, color='black')\n",
        "        ),\n",
        "        hoverinfo='text',\n",
        "        text=all_nodes,\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    edge_traces = []\n",
        "    max_weight = max(weights) if weights else 1\n",
        "\n",
        "    for s, t, w in zip(source, target, weights):\n",
        "        if w <= 0:\n",
        "            continue\n",
        "        x0, x1 = node_indices[s], node_indices[t]\n",
        "        x_middle = (x0 + x1) / 2\n",
        "        height = abs(x1 - x0) * 0.3\n",
        "\n",
        "        bezier_x = [x0, x_middle, x1]\n",
        "        bezier_y = [0, height, 0]\n",
        "\n",
        "        edge_color = 'LightSkyBlue'\n",
        "        width = 1 + (w / max_weight) * 4\n",
        "\n",
        "        edge_trace = go.Scatter(\n",
        "            x=bezier_x,\n",
        "            y=bezier_y,\n",
        "            mode='lines',\n",
        "            line=dict(width=width, color=edge_color, shape='spline'),\n",
        "            hoverinfo='text',\n",
        "            text=f'{s} → {t}<br>Peso: {w:.2f}',\n",
        "            showlegend=False\n",
        "        )\n",
        "        edge_traces.append(edge_trace)\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for edge in edge_traces:\n",
        "        fig.add_trace(edge)\n",
        "    fig.add_trace(node_trace)\n",
        "\n",
        "    annotations = []\n",
        "    for i, name in enumerate(all_nodes):\n",
        "        annotations.append(\n",
        "            dict(\n",
        "                x=x[i],\n",
        "                y=-0.3,\n",
        "                text=name,\n",
        "                showarrow=False,\n",
        "                textangle=65,\n",
        "                font=dict(size=12),\n",
        "                xanchor='center',\n",
        "                yanchor='top'\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        title_font_size=20,\n",
        "        showlegend=False,\n",
        "        plot_bgcolor='white',\n",
        "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        margin=dict(t=60, b=20, l=20, r=20),\n",
        "        height=500,\n",
        "        annotations=annotations\n",
        "    )\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bD1yQySzh9i"
      },
      "outputs": [],
      "source": [
        "def get_inter_cluster_weights(G):\n",
        "    # Encontrar todos los clusters\n",
        "    node_clusters_dict = nx.get_node_attributes(G, 'cluster_name')\n",
        "    # Encontrar todos los nombres de clusters únicos\n",
        "    unique_clusters = set(node_clusters_dict.values())\n",
        "    # Encontrar todas las combinaciones de 2 clusters únicos\n",
        "    cluster_combinations = list(combinations(unique_clusters, 2))\n",
        "\n",
        "    source = []\n",
        "    target = []\n",
        "    source_target_weight = []\n",
        "\n",
        "    # Iterar sobre cada par de clusters en las combinaciones\n",
        "    for cluster1, cluster2 in cluster_combinations: # Usamos cluster1, cluster2 directamente del tuple\n",
        "        # Filtrar nodos que pertenecen a cluster1 usando el diccionario original\n",
        "        nodes_cluster1 = [node for node, cluster_value in node_clusters_dict.items() if cluster_value == cluster1]\n",
        "        # Filtrar nodos que pertenecen a cluster2 usando el diccionario original\n",
        "        nodes_cluster2 = [node for node, cluster_value in node_clusters_dict.items() if cluster_value == cluster2]\n",
        "\n",
        "        # Añadir clusters a resultados\n",
        "        source.append(cluster1)\n",
        "        target.append(cluster2) # target should also be a single cluster name per pair\n",
        "\n",
        "        # Añadir número de conecciones.\n",
        "        w = 0\n",
        "        for node1 in nodes_cluster1:\n",
        "            for node2 in nodes_cluster2:\n",
        "                # Check for edges in both directions\n",
        "                if G.has_edge(node1, node2):\n",
        "                    w += G[node1][node2]['weight']\n",
        "                if G.has_edge(node2, node1): # Consider edges in the reverse direction as well\n",
        "                    w += G[node2][node1]['weight']\n",
        "        source_target_weight.append(int(w))\n",
        "\n",
        "    return source, target, source_target_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yOA8xngwQfCK",
        "outputId": "a5086ecb-2ba7-4738-f7e1-cd8d1fc5bb01"
      },
      "outputs": [],
      "source": [
        "for platform, G in platform_networks.items():\n",
        "    print(f\"--- Analizando redes para la plataforma: {platform} ---\")\n",
        "\n",
        "    # Crear carpeta para la plataforma\n",
        "    folder_path = os.path.join(\"app\", \"static\", \"plots\", \"platforms\", platform)\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    pos = nx.spring_layout(G, k=0.1)\n",
        "\n",
        "    # Graficar y guardar network plot\n",
        "    network_fig = plot_network(G, pos, title=f\"Red de Usuarios de {platform} por similtud discursiva\")\n",
        "    network_file = os.path.join(folder_path, \"network_plot.html\")\n",
        "    network_fig.write_html(network_file)\n",
        "    network_fig.show()\n",
        "\n",
        "    # Graficar y guardar density plot\n",
        "    density_fig = plot_node_density(G, pos, title=f\"Plot de densidad de usuarios de {platform} por similtud discursiva\")\n",
        "    density_file = os.path.join(folder_path, \"density_plot.html\")\n",
        "    density_fig.write_html(density_file)\n",
        "    density_fig.show()\n",
        "\n",
        "    # arc_fig = plot_arc_diagram(*get_inter_cluster_weights(G), title=f\"Diagrama de arcos para comunidades de {platform} por similtud discursiva\")\n",
        "    # arc_file = os.path.join(folder_path, \"arc_diagram.html\")\n",
        "    # arc_fig.write_html(arc_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DD_sWfJpJQV",
        "outputId": "0f1b93d7-1d1a-4a53-a0a7-142fbf258829"
      },
      "outputs": [],
      "source": [
        "for platform, G in individual_networks.items():\n",
        "    print(f\"--- Analizando redes individuales para la plataforma: {platform} ---\")\n",
        "    for category, user_graphs in G.items():\n",
        "        print(f\"  Categoría: {category}\")\n",
        "        for user, ego_graph in user_graphs.items():\n",
        "            if ego_graph.number_of_nodes() > 1:\n",
        "                print(f\"    Graficando red de: {user} ({ego_graph.number_of_nodes()} nodos, {ego_graph.number_of_edges()} aristas)\")\n",
        "                pos = nx.spring_layout(ego_graph, k=0.1)\n",
        "                plot_network(ego_graph, pos, highlight_node=user, title=f\"{platform} - Red de similtud discursiva de {user}\").show()\n",
        "                plot_node_density(ego_graph, pos, title=f\"{platform} - Red de densidad discursiva de {user}\").show()\n",
        "            else:\n",
        "                print(f\"{user} solo tiene un nodo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK8yY_kQJEll"
      },
      "source": [
        "## 3.2 Medir Metricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsKOl5JtG3QB"
      },
      "source": [
        "### 3.2.1 Cohesion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xzTFGZCJEll"
      },
      "outputs": [],
      "source": [
        "def calculate_cohesion_metrics(G: nx.DiGraph) -> dict:\n",
        "    metrics = {}\n",
        "\n",
        "    # 1. Densidad\n",
        "    dens = nx.density(G)\n",
        "    metrics['densidad'] = dens\n",
        "\n",
        "    # 2. Clustering promedio\n",
        "    und = G.to_undirected()\n",
        "    clust = nx.average_clustering(und)\n",
        "    metrics['clustering_promedio'] = clust\n",
        "\n",
        "    # 3. Transitividad\n",
        "    trans = nx.transitivity(und)\n",
        "    metrics['transitividad'] = trans\n",
        "\n",
        "    # 4. Reciprocidad\n",
        "    try:\n",
        "        rec = nx.reciprocity(G)\n",
        "        metrics['reciprocidad'] = rec\n",
        "    except Exception:\n",
        "        metrics['reciprocidad'] = np.nan\n",
        "\n",
        "    # 5. Longitud media del camino más corto\n",
        "    if nx.is_strongly_connected(G):\n",
        "        avg_sp = nx.average_shortest_path_length(G, weight=None)\n",
        "        metrics['camino_mas_corto_medio'] = avg_sp\n",
        "        sub = G\n",
        "    else:\n",
        "        comp = max(nx.strongly_connected_components(G), key=len)\n",
        "        sub = G.subgraph(comp)\n",
        "        try:\n",
        "            avg_sp = nx.average_shortest_path_length(sub, weight=None)\n",
        "            metrics['camino_mas_corto_medio'] = avg_sp\n",
        "        except:\n",
        "            metrics['camino_mas_corto_medio'] = np.nan\n",
        "\n",
        "    # 6. Diámetro\n",
        "    try:\n",
        "        diam = nx.diameter(sub)\n",
        "        metrics['diametro'] = diam\n",
        "    except Exception:\n",
        "        metrics['diametro'] = np.nan\n",
        "\n",
        "    # 7. Conectividad\n",
        "    try:\n",
        "        node_conn = nx.node_connectivity(und)\n",
        "        edge_conn = nx.edge_connectivity(und)\n",
        "        metrics['conectividad_nodos'] = node_conn\n",
        "        metrics['conectividad_aristas'] = edge_conn\n",
        "    except Exception:\n",
        "        metrics['conectividad_nodos'] = np.nan\n",
        "        metrics['conectividad_aristas'] = np.nan\n",
        "\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhiKSs4SycF0",
        "outputId": "36b25256-cd6c-4016-96d1-8e133608607d"
      },
      "outputs": [],
      "source": [
        "cohesion_results = {}\n",
        "for platform, category_data in individual_networks.items():\n",
        "    cohesion_results[platform] = {}\n",
        "    for category, networks in category_data.items():\n",
        "        cohesion_results[platform][category] = {}\n",
        "        for user, network in networks.items():\n",
        "            print(f\"Calculando metricas de cohesión para {platform} - {category} - {user}\")\n",
        "            metrics = calculate_cohesion_metrics(network)\n",
        "            cohesion_results[platform][category][user] = metrics\n",
        "\n",
        "flat_cohesion_data = []\n",
        "for platform, category_data in cohesion_results.items():\n",
        "    for category, user_data in category_data.items():\n",
        "        for user, metrics in user_data.items():\n",
        "            entry = {'platform': platform, 'category': category, 'username': user}\n",
        "            entry.update(metrics)\n",
        "            flat_cohesion_data.append(entry)\n",
        "\n",
        "# Crear el df\n",
        "cohesion_df = pd.DataFrame(flat_cohesion_data)\n",
        "\n",
        "# Mostrar el df\n",
        "display(cohesion_df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnDqeLafJEll"
      },
      "source": [
        "### 3.2.2 Polarización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFkwYF0MhstK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_entropy(partition_labels: list) -> float:\n",
        "    # Contar la frecuencia de cada etiqueta de cluster\n",
        "    value_counts = pd.Series(partition_labels).value_counts()\n",
        "    # Calcular las probabilidades\n",
        "    probabilities = value_counts / len(partition_labels)\n",
        "    # Calcular la entropía\n",
        "    entropy_value = -np.sum(probabilities * np.log2(probabilities + 1e-9)) # Add epsilon for log(0)\n",
        "    return entropy_value\n",
        "\n",
        "def calculate_e_i_index(graph: nx.Graph, partition_labels: list) -> float:\n",
        "    # Mapear nodos a sus etiquetas de cluster\n",
        "    node_to_label = {node: label for node, label in zip(graph.nodes(), partition_labels)}\n",
        "\n",
        "    # Contar aristas inter-cluster (E) y intra-cluster (I)\n",
        "    e_edges = 0\n",
        "    i_edges = 0\n",
        "\n",
        "    for u, v in graph.edges():\n",
        "        if node_to_label[u] != node_to_label[v]:\n",
        "            e_edges += 1\n",
        "        else:\n",
        "            i_edges += 1\n",
        "\n",
        "    # Calcular el índice E-I\n",
        "    # Evitar división por cero si no hay aristas\n",
        "    if (e_edges + i_edges) == 0:\n",
        "        return np.nan\n",
        "    else:\n",
        "        return (e_edges - i_edges) / (e_edges + i_edges)\n",
        "\n",
        "def analyze_polarization_metrics(graph: nx.Graph, cluster_name: str) -> dict:\n",
        "    metrics = {}\n",
        "\n",
        "    # Obtener las etiquetas de cluster para los nodos\n",
        "    node_attributes = nx.get_node_attributes(graph, cluster_name)\n",
        "\n",
        "    # Si no hay información del cluster, no se pueden calcular las métricas de polarización\n",
        "    if not node_attributes:\n",
        "        print(f\"No hay atributo '{cluster_name}' en los nodos del grafo.\")\n",
        "        return metrics\n",
        "\n",
        "    # Asegurarse de que las etiquetas estén en el mismo orden que los nodos\n",
        "    ordered_labels = [node_attributes.get(node) for node in graph.nodes()]\n",
        "\n",
        "    # Calcular Entropía\n",
        "    metrics['entropia'] = calculate_entropy(ordered_labels)\n",
        "\n",
        "    # Calcular Índice E-I\n",
        "    undirected_graph = graph.to_undirected()\n",
        "    metrics['indice E-I'] = calculate_e_i_index(undirected_graph, ordered_labels)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# --- Aplicación de la función de análisis de polarización ---\n",
        "polarization_results = {}\n",
        "for platform, category_data in individual_networks.items():\n",
        "    polarization_results[platform] = {}\n",
        "    for category, networks in category_data.items():\n",
        "        polarization_results[platform][category] = {}\n",
        "        for user, network in networks.items():\n",
        "            print(f\"Calculando metricas de polarización para {platform} - {category} - {user}\")\n",
        "            # Utilizamos 'cluster_name' como el atributo de nodo para la partición\n",
        "            metrics = analyze_polarization_metrics(network, 'cluster_name')\n",
        "            polarization_results[platform][category][user] = metrics\n",
        "\n",
        "# Aplanar los resultados en un DataFrame\n",
        "flat_polarization_data = []\n",
        "for platform, category_data in polarization_results.items():\n",
        "    for category, user_data in category_data.items():\n",
        "        for user, metrics in user_data.items():\n",
        "            entry = {'platform': platform, 'category': category, 'username': user}\n",
        "            entry.update(metrics)\n",
        "            flat_polarization_data.append(entry)\n",
        "\n",
        "# Crear el DataFrame\n",
        "polarization_df = pd.DataFrame(flat_polarization_data)\n",
        "\n",
        "# Mostrar el DataFrame\n",
        "display(polarization_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJoNecAcYZvt"
      },
      "source": [
        "# 4 Relacionar las comunidades con sus métricas de influencia (likes, views, comentarios)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbDaBi4nfraW"
      },
      "outputs": [],
      "source": [
        "# Colores representativos de los partidos\n",
        "PARTY_COLORS = {\n",
        "    'Jorge Álvarez Máynez': '#FF5E00',\n",
        "    'Xóchitl Gálvez': '#005BAC',\n",
        "    'Claudia Sheinbaum': '#A6192E'\n",
        "}\n",
        "\n",
        "def save_wordcloud_image(text, filepath):\n",
        "    wc = WordCloud(width=600, height=400, background_color='white', colormap='viridis').generate(text)\n",
        "    wc.to_file(filepath)\n",
        "    print(f\"WordCloud guardada en: {filepath}\")\n",
        "\n",
        "def plot_cluster_analysis(candidate_counts, title, num_users, avg_interactions, top_users):\n",
        "    pie = go.Pie(\n",
        "        labels=candidate_counts.index,\n",
        "        values=candidate_counts.values,\n",
        "        marker=dict(colors=[PARTY_COLORS.get(name, '#888888') for name in candidate_counts.index],\n",
        "                    line=dict(color='#111111', width=1)),\n",
        "        textinfo='percent+label',\n",
        "        pull=[0.02] * len(candidate_counts),\n",
        "        opacity=0.9,\n",
        "        domain=dict(x=[0.15, 0.85], y=[0.3, 1.0])  \n",
        "    )\n",
        "\n",
        "    users_text = \", \".join(top_users)\n",
        "    stats_text = (\n",
        "        f\"Usuarios: {num_users}<br>\"\n",
        "        f\"Prom. Interacciones: {avg_interactions:.2f}<br>\"\n",
        "        f\"Principales usuarios: {users_text}<br>\"\n",
        "    )\n",
        "\n",
        "    layout = go.Layout(\n",
        "        title=dict(\n",
        "            text=f\"{title}\",\n",
        "            x=0.5,\n",
        "            xanchor=\"center\",\n",
        "            font=dict(size=18)\n",
        "        ),\n",
        "        width=1250,\n",
        "        height=750,  \n",
        "        paper_bgcolor='rgb(246, 248, 250)',\n",
        "        plot_bgcolor='#FFFFFF',\n",
        "        annotations=[\n",
        "            dict(\n",
        "                text=stats_text,\n",
        "                x=0.5,\n",
        "                y=0.1, \n",
        "                showarrow=False,\n",
        "                font=dict(size=13),\n",
        "                align=\"center\",\n",
        "                xanchor=\"center\",\n",
        "                yanchor=\"top\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    fig = go.Figure(data=[pie], layout=layout)\n",
        "    return fig\n",
        "\n",
        "def plot_and_save_cluster_metrics(df, platform):\n",
        "    clusters = df['cluster_name'].unique()\n",
        "\n",
        "    for c_name in clusters:\n",
        "        cluster_df = df[df['cluster_name'] == c_name].copy()\n",
        "        cluster_candidates = cluster_df['candidate_name'].tolist()\n",
        "        candidate_counts = pd.Series(cluster_candidates).value_counts()\n",
        "        avg_interactions = cluster_df['num_interaction'].mean()\n",
        "        num_users = len(cluster_df)\n",
        "        top_users = (\n",
        "            cluster_df.sort_values(by='num_interaction', ascending=False)\n",
        "            .head(5)['username']\n",
        "            .tolist()\n",
        "        )\n",
        "\n",
        "        # Texto combinado para WordCloud\n",
        "        wordcloud_text = \" \".join(cluster_df['clean_text'].dropna().astype(str).tolist())\n",
        "\n",
        "        output_dir = f\"app/static/plots/platforms/{platform}/clusters/\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Guardar WordCloud por separado\n",
        "        wc_filepath = os.path.join(output_dir, f\"wordcloud_{c_name}.png\")\n",
        "        save_wordcloud_image(wordcloud_text, wc_filepath)\n",
        "\n",
        "        title = f'Análisis de Comunidad: {c_name}'\n",
        "        fig = plot_cluster_analysis(candidate_counts, title, num_users, avg_interactions, top_users)\n",
        "\n",
        "        output_path = os.path.join(output_dir, f\"cluster_{c_name}.html\")\n",
        "        fig.write_html(output_path)\n",
        "        print(f\"Gráfico guardado en: {output_path}\")\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "# Ejecutar para cada plataforma\n",
        "for platform, df_platform in platform_dfs.items():\n",
        "    print(f\"--- Analizando Clusters para la Plataforma: {platform} ---\")\n",
        "    plot_and_save_cluster_metrics(df_platform, platform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT8iqsd07XbX"
      },
      "source": [
        "# 5 Documentar el nivel de fragmentación del espacio discursivo electoral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQg7RdlNJjNm"
      },
      "outputs": [],
      "source": [
        "def plot_cohesion(df: pd.DataFrame, title):\n",
        "    object_columns = ['platform', 'category', 'username']\n",
        "    numeric_columns = [col for col in df.columns if col not in object_columns]\n",
        "\n",
        "    num_metrics = len(numeric_columns)\n",
        "    if num_metrics <= 2:\n",
        "        rows, cols = 1, num_metrics\n",
        "    elif num_metrics <= 4:\n",
        "        rows, cols = 2, 2\n",
        "    elif num_metrics <= 6:\n",
        "        rows, cols = 2, 3\n",
        "    elif num_metrics <= 9:\n",
        "        rows, cols = 3, 3\n",
        "    else:\n",
        "        rows = (num_metrics + 2) // 3\n",
        "        cols = 3\n",
        "\n",
        "    subplot_titles = [f\"<b>{col.replace('_', ' ').title()}</b>\" for col in numeric_columns]\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=rows,\n",
        "        cols=cols,\n",
        "        subplot_titles=subplot_titles,\n",
        "        vertical_spacing=0.1 if rows > 1 else 0,\n",
        "        horizontal_spacing=0.07 if cols > 1 else 0\n",
        "    )\n",
        "\n",
        "    bar_colors = plotly.colors.qualitative.Plotly\n",
        "\n",
        "    for i, metric in enumerate(numeric_columns):\n",
        "        current_row = i // cols + 1\n",
        "        current_col = i % cols + 1\n",
        "\n",
        "        df_sorted = df.sort_values(by=metric, ascending=False)\n",
        "\n",
        "        hover_texts = [\n",
        "            f\"<b>Usuario:</b> {user}<br>\"\n",
        "            f\"<b>{metric.replace('_', ' ').title()}:</b> {value:.2f} \"\n",
        "            f\"{'(' + platform + ')' if 'platform' in df_sorted.columns else ''}\"\n",
        "            for user, value, platform in zip(\n",
        "                df_sorted['username'],\n",
        "                df_sorted[metric],\n",
        "                df_sorted['platform'] if 'platform' in df_sorted.columns else ['N/A'] * len(df_sorted)\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=df_sorted['username'],\n",
        "                y=df_sorted[metric],\n",
        "                name=metric.replace('_', ' ').title(),\n",
        "                marker_color=bar_colors[i % len(bar_colors)],\n",
        "                hovertext=hover_texts,\n",
        "                hoverinfo='text',\n",
        "                text=[f\"{val:.2f}\" if isinstance(val, float) else f\"{val}\" for val in df_sorted[metric]],\n",
        "                textposition='none',\n",
        "            ),\n",
        "            row=current_row,\n",
        "            col=current_col\n",
        "        )\n",
        "\n",
        "        fig.update_xaxes(type='category', tickangle=45, row=current_row, col=current_col)\n",
        "        fig.update_yaxes(title_text=\"Value\", showgrid=True, gridwidth=0.5, gridcolor='LightGrey', row=current_row, col=current_col)\n",
        "\n",
        "    fig_height = 800 * rows\n",
        "    fig_width = 600 * cols if cols > 1 else 700\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=f\"{title}\",\n",
        "        title_x=0.5,\n",
        "        height=max(600, fig_height),\n",
        "        width=max(800, fig_width),\n",
        "        showlegend=False,\n",
        "        template=\"plotly_white\",\n",
        "        font=dict(family=\"Arial, sans-serif\", size=12, color=\"black\"),\n",
        "        paper_bgcolor='rgb(246, 248, 250)',\n",
        "        plot_bgcolor='rgba(240,240,240,0.95)',\n",
        "        margin=dict(l=60, r=60, t=80, b=120)\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "plot_cohesion_df = cohesion_df.copy()\n",
        "plot_cohesion_df['username'] = cohesion_df['username'] + ' (' + cohesion_df['platform'] + ')'\n",
        "\n",
        "\n",
        "os.makedirs('app/static/plots/cohesion', exist_ok=True)\n",
        "\n",
        "category_list = plot_cohesion_df['category'].unique()\n",
        "for category in category_list:\n",
        "    category_df = plot_cohesion_df[plot_cohesion_df['category'] == category]\n",
        "    title = f\"<b>Comparación de metricas de cohesión para: {category}</b>\"\n",
        "    fig = plot_cohesion(category_df, title)\n",
        "    fig.show()\n",
        "    output_path = f\"app/static/plots/cohesion/{category}.html\"\n",
        "    fig.write_html(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwCkznvYbQgV"
      },
      "outputs": [],
      "source": [
        "def plot_polarization(df: pd.DataFrame, title):\n",
        "\n",
        "    object_columns = ['platform', 'category', 'username']\n",
        "    numeric_columns = ['entropia', 'indice E-I'] # Especificamos las columnas numéricas de polarización\n",
        "\n",
        "    num_metrics = len(numeric_columns)\n",
        "    rows, cols = 1, num_metrics\n",
        "\n",
        "    subplot_titles = [f\"{col.replace('_', ' ').title()}</b>\" for col in numeric_columns]\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=rows,\n",
        "        cols=cols,\n",
        "        subplot_titles=subplot_titles,\n",
        "        vertical_spacing=0.1 if rows > 1 else 0,\n",
        "        horizontal_spacing=0.07 if cols > 1 else 0\n",
        "    )\n",
        "\n",
        "    bar_colors = plotly.colors.qualitative.Plotly\n",
        "\n",
        "    for i, metric in enumerate(numeric_columns):\n",
        "        current_row = i // cols + 1\n",
        "        current_col = i % cols + 1\n",
        "\n",
        "        # Ordenar por la métrica para una mejor visualización\n",
        "        df_sorted = df.sort_values(by=metric, ascending=False)\n",
        "\n",
        "        hover_texts = [\n",
        "            f\"Usuario:</b> {user}<br>\"\n",
        "            f\"{metric.replace('_', ' ').title()}:</b> {value:.4f} \" # Formato a 4 decimales\n",
        "            f\" ({platform})\"\n",
        "            for user, value, platform in zip(\n",
        "                df_sorted['username'],\n",
        "                df_sorted[metric],\n",
        "                df_sorted['platform']\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=df_sorted['username'],\n",
        "                y=df_sorted[metric],\n",
        "                name=metric.replace('_', ' ').title(),\n",
        "                marker_color=bar_colors[i % len(bar_colors)],\n",
        "                hovertext=hover_texts,\n",
        "                hoverinfo='text',\n",
        "                text=[f\"{val:.4f}\" if isinstance(val, float) else f\"{val}\" for val in df_sorted[metric]],\n",
        "                textposition='none', # Ocultar texto en la barra, usar hover\n",
        "            ),\n",
        "            row=current_row,\n",
        "            col=current_col\n",
        "        )\n",
        "\n",
        "        fig.update_xaxes(type='category', tickangle=45, row=current_row, col=current_col)\n",
        "        fig.update_yaxes(title_text=\"Value\", showgrid=True, gridwidth=0.5, gridcolor='LightGrey', row=current_row, col=current_col)\n",
        "\n",
        "    fig_height = 600\n",
        "    fig_width = 1200\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=f\"{title}</b>\",\n",
        "        title_x=0.5,\n",
        "        height=fig_height,\n",
        "        width=fig_width,\n",
        "        showlegend=False,\n",
        "        template=\"plotly_white\",\n",
        "        font=dict(family=\"Arial, sans-serif\", size=12, color=\"black\"),\n",
        "        paper_bgcolor='rgb(246, 248, 250)',\n",
        "        plot_bgcolor='rgba(240,240,240,0.95)',\n",
        "        margin=dict(l=60, r=60, t=80, b=120)\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "plot_polarization_df = polarization_df.copy()\n",
        "plot_polarization_df['username'] = polarization_df['username'] + ' (' + polarization_df['platform'] + ')'\n",
        "\n",
        "os.makedirs('app/static/plots/polarization', exist_ok=True)\n",
        "\n",
        "category_list = plot_polarization_df['category'].unique()\n",
        "for category in category_list:\n",
        "    category_df = plot_polarization_df[plot_polarization_df['category'] == category]\n",
        "    title = f\"Comparación de métricas de polarización para: {category}\"\n",
        "    fig = plot_polarization(category_df, title)\n",
        "    fig.show()\n",
        "    output_path = f\"app/static/plots/polarization/{category}.html\"\n",
        "    fig.write_html(output_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Tareas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
